{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGMD S-17 Summer 2020 - Project PopBot\n",
    "\n",
    "## Team Members:\n",
    ">- Chris Crane\n",
    ">- Robert Clapp\n",
    ">- Andrew Pham\n",
    ">- James Sun\n",
    "\n",
    "## Project Objectives:\n",
    "**The objective of Team PupBot for this fianl DGMD S-17 Summer 2020 project include: \n",
    "- Object Detection - Development of code to provide object detection on team member robots\n",
    "- Object Avoidance - Development of code to avoid specific objects \n",
    "- Object Following - Development of code to follow specific objects\n",
    "- Object Retreat - Development of code to retreat or run from specific objects\n",
    "\n",
    "### Notes:\n",
    "####**This notebook is not designed to be run in its entirety and kernel/robot restarts are required.** \n",
    "\n",
    "####**Restart kernel between the running of each section - Kernel restarts will be annotated in section headings**\n",
    "\n",
    "1. Team PupBot's robots must first be trained to recognize objects in each team member's environment by running the code in the **Data Collection** section. \n",
    "2. All newly captured images in Data Collection must be saved to the `dataset` folder and subfolders.\n",
    "3. After finishing data collection, run the **Train Model** section. Trained model data is saved in the `best_model.pth` file\n",
    "4. After training the model, run the **Project Code Execute** section to demonstrate the objectives of the Team PupBot project.\n",
    "5. It may be necessary to reboot the robot between data collection and the main entry point (if kernel restart does not work) to get the camera resource release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############################################################################\n",
    "## Data Collection \n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Display live camera feed\n",
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "from uuid import uuid1\n",
    "import os\n",
    "\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)  # this width and height doesn't necessarily have to match the camera\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "blocked_dir = 'dataset/blocked'\n",
    "free_dir = 'dataset/free'\n",
    "\n",
    "# we have this \"try/except\" statement because these next functions can throw an error if the directories exist already\n",
    "try:\n",
    "    os.makedirs(free_dir)\n",
    "    os.makedirs(blocked_dir)\n",
    "except FileExistsError:\n",
    "    print('Directories not created becasue they already exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "button_layout = widgets.Layout(width='128px', height='64px')\n",
    "free_button = widgets.Button(description='add free', button_style='success', layout=button_layout)\n",
    "blocked_button = widgets.Button(description='add blocked', button_style='danger', layout=button_layout)\n",
    "free_count = widgets.IntText(layout=button_layout, value=len(os.listdir(free_dir)))\n",
    "blocked_count = widgets.IntText(layout=button_layout, value=len(os.listdir(blocked_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def save_snapshot(directory):\n",
    "    image_path = os.path.join(directory, str(uuid1()) + '.jpg')\n",
    "    with open(image_path, 'wb') as f:\n",
    "        f.write(image.value)\n",
    "\n",
    "def save_free():\n",
    "    global free_dir, free_count\n",
    "    save_snapshot(free_dir)\n",
    "    free_count.value = len(os.listdir(free_dir))\n",
    "    \n",
    "def save_blocked():\n",
    "    global blocked_dir, blocked_count\n",
    "    save_snapshot(blocked_dir)\n",
    "    blocked_count.value = len(os.listdir(blocked_dir))\n",
    "    \n",
    "free_button.on_click(lambda x: save_free())\n",
    "blocked_button.on_click(lambda x: save_blocked())\n",
    "\n",
    "display(image)\n",
    "display(widgets.HBox([free_count, free_button]))\n",
    "display(widgets.HBox([blocked_count, blocked_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################################################################\n",
    "## Train Model (Kernel Reboot Required Here)\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset instance\n",
    "dataset = datasets.ImageFolder(\n",
    "    'dataset',\n",
    "    transforms.Compose([\n",
    "        transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 50, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create data loaders to load data in batches\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "model = models.alexnet(pretrained=True)\n",
    "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 2)\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Train the neural network\n",
    "NUM_EPOCHS = 30\n",
    "BEST_MODEL_PATH = 'best_model.pth'\n",
    "best_accuracy = 0.0\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    for images, labels in iter(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    test_error_count = 0.0\n",
    "    for images, labels in iter(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_error_count += float(torch.sum(torch.abs(labels - outputs.argmax(1))))\n",
    "    \n",
    "    test_accuracy = 1.0 - float(test_error_count) / float(len(test_dataset))\n",
    "    print('%d: %f' % (epoch, test_accuracy))\n",
    "    if test_accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        best_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################################################\n",
    "## Project Code Execute (Kernel Reboot and Possible Robot Reboot Required Here)\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "import traitlets\n",
    "from jetbot import Heartbeat, ObjectDetector, Camera, Robot, bgr8_to_jpeg\n",
    "import json\n",
    "\n",
    "# Load COCO object detection labels\n",
    "with open('coco_label_map.json', 'r') as f:\n",
    "    coco_labels = json.load(f)\n",
    "f.close()\n",
    "\n",
    "#for label in coco_labels:\n",
    "#    print(str(label['id']) + ': ' + label['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')\n",
    "\n",
    "camera = Camera.instance(width=300, height=300)\n",
    "\n",
    "robot = Robot()\n",
    "robot.stop;\n",
    "\n",
    "heartbeat = Heartbeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision_model = torchvision.models.alexnet(pretrained=False)\n",
    "collision_model.classifier[6] = torch.nn.Linear(collision_model.classifier[6].in_features, 2)\n",
    "collision_model.load_state_dict(torch.load('best_model.pth'))\n",
    "device = torch.device('cuda')\n",
    "collision_model = collision_model.to(device)\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c31587457f44cd6be83fff4db7d4d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg', height='300', width='300'), FloatSlider(value=0.â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "blocked_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = widgets.IntText(value=1, description='tracked label') # Looking for a Person (value = 1)\n",
    "speed_widget = widgets.FloatSlider(value=0.30, min=0.0, max=1.0, step=0.05, description='speed')\n",
    "turn_gain_widget = widgets.FloatSlider(value=0.8, min=0.0, max=2.0, description='turn gain')\n",
    "stop_button = widgets.Button(description='Stop', button_style='danger')\n",
    "start_button = widgets.Button(description='Restart', button_style='success')\n",
    "det_center_text = widgets.Text(value='')\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget, blocked_widget, stop_button, start_button]),\n",
    "    widgets.HBox([label_widget,det_center_text]),\n",
    "    speed_widget,\n",
    "    turn_gain_widget\n",
    "]))\n",
    "\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "def get_coco_label(label_id):\n",
    "    label = 'UNKNOWN'\n",
    "    for l in coco_labels:\n",
    "        if l['id'] == label_id:\n",
    "            label = l['label']\n",
    "            break\n",
    "    return label\n",
    "\n",
    "#Computes the center x, y coordinates of the object\n",
    "def detection_center(detection):\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "#Computes the length of the 2D vector\n",
    "def norm(vec):\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "#Finds the detection closest to the image center\n",
    "def closest_detection(detections):\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "\n",
    "#Process camera input, find detections, follow target if identidied\n",
    "def execute(change):\n",
    "    image = change['new']\n",
    "    \n",
    "    # execute collision model to determine if blocked\n",
    "    collision_output = collision_model(preprocess(image)).detach().cpu()\n",
    "    prob_blocked = float(F.softmax(collision_output.flatten(), dim=0)[0])\n",
    "    blocked_widget.value = prob_blocked\n",
    "    \n",
    "    # turn left if blocked\n",
    "    if prob_blocked > 0.5:\n",
    "        robot.left(0.4)\n",
    "        image_widget.value = bgr8_to_jpeg(image)\n",
    "        return\n",
    "        \n",
    "    # compute all detected objects\n",
    "    detections = model(image)\n",
    "    \n",
    "    # draw all detections on image\n",
    "    for det in detections[0]:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "        #cv2.putText(image, get_coco_label(det['label']), (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # select detections that match selected class label\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "    \n",
    "    # get detection closest to center of field of view and draw it\n",
    "    det = closest_detection(matching_detections)\n",
    "    if det is not None:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 5)\n",
    "        #cv2.putText(image, get_coco_label(det['label']), (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # otherwise go forward if no target detected\n",
    "    if det is None:\n",
    "        robot.forward(float(speed_widget.value))\n",
    "        det_center_text.value = ''\n",
    "        \n",
    "    # otherwsie steer towards target\n",
    "    else:\n",
    "        # move robot forward and steer proportional target's x-distance from center\n",
    "        center = detection_center(det)\n",
    "        det_center_text.value = str(center)\n",
    "        robot.set_motors(\n",
    "            float(speed_widget.value + turn_gain_widget.value * center[0]),\n",
    "            float(speed_widget.value - turn_gain_widget.value * center[0])\n",
    "        )\n",
    "    \n",
    "    # update image widget\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "\n",
    "def stop(change):\n",
    "    camera.unobserve_all()\n",
    "    time.sleep(0.5)\n",
    "    robot.stop()\n",
    "\n",
    "def restart(change):\n",
    "    camera.unobserve_all()\n",
    "    camera.observe(execute, names='value')\n",
    "\n",
    "# this function will be called when heartbeat 'alive' status changes\n",
    "def handle_heartbeat_status(change):\n",
    "    if change['new'] == Heartbeat.Status.dead:\n",
    "        robot.stop()\n",
    "        \n",
    "stop_button.on_click(stop)\n",
    "start_button.on_click(restart)\n",
    "\n",
    "#execute({'new': camera.value})\n",
    "camera.unobserve_all()\n",
    "camera.observe(execute, names='value')\n",
    "heartbeat.observe(handle_heartbeat_status, names='status')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
